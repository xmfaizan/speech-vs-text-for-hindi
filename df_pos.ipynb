{
  "metadata": {
    "kernelspec": {
      "language": "python",
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.13",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "kaggle": {
      "accelerator": "none",
      "dataSources": [
        {
          "sourceId": 8784850,
          "sourceType": "datasetVersion",
          "datasetId": 5281104
        }
      ],
      "dockerImageVersionId": 30733,
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook",
      "isGpuEnabled": false
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "source": [
        "\n",
        "# IMPORTANT: RUN THIS CELL IN ORDER TO IMPORT YOUR KAGGLE DATA SOURCES\n",
        "# TO THE CORRECT LOCATION (/kaggle/input) IN YOUR NOTEBOOK,\n",
        "# THEN FEEL FREE TO DELETE THIS CELL.\n",
        "# NOTE: THIS NOTEBOOK ENVIRONMENT DIFFERS FROM KAGGLE'S PYTHON\n",
        "# ENVIRONMENT SO THERE MAY BE MISSING LIBRARIES USED BY YOUR\n",
        "# NOTEBOOK.\n",
        "\n",
        "import os\n",
        "import sys\n",
        "from tempfile import NamedTemporaryFile\n",
        "from urllib.request import urlopen\n",
        "from urllib.parse import unquote, urlparse\n",
        "from urllib.error import HTTPError\n",
        "from zipfile import ZipFile\n",
        "import tarfile\n",
        "import shutil\n",
        "\n",
        "CHUNK_SIZE = 40960\n",
        "DATA_SOURCE_MAPPING = 'gggsdsd:https%3A%2F%2Fstorage.googleapis.com%2Fkaggle-data-sets%2F5281104%2F8784850%2Fbundle%2Farchive.zip%3FX-Goog-Algorithm%3DGOOG4-RSA-SHA256%26X-Goog-Credential%3Dgcp-kaggle-com%2540kaggle-161607.iam.gserviceaccount.com%252F20240709%252Fauto%252Fstorage%252Fgoog4_request%26X-Goog-Date%3D20240709T152624Z%26X-Goog-Expires%3D259200%26X-Goog-SignedHeaders%3Dhost%26X-Goog-Signature%3D59e243152c5ab0356f916d603479f77e2cf8bed7406393c9c1353cc8c8a23b98a145b650c827f1aa02ec6ae68318eab63ecd526d3a7dcf67bcc9231411a24b59be6c0be88a676ce1ec2ba7e88329c054f34ab241af214cdb1bc45fde33de2f09bf897fe052739caabdd37964a1a00a4e08cfd842a9d904ecbe0a14e6167738c847de1e454a097b9d771bb920ae2471a618c50beb7557b447c80e5197647ec82299f133f3e06e73c3663c2836f6c3b779b27ff3ba2c3f89d81d194eb59a586fb75842c693c4c092ca7154df7bdc8b128b7485ef77e85f3f4057b2d7a405eff36d4c99a0cb5f79887443e69d246ca14018cccc31c4f0517549429426029ced1ee1'\n",
        "\n",
        "KAGGLE_INPUT_PATH='/kaggle/input'\n",
        "KAGGLE_WORKING_PATH='/kaggle/working'\n",
        "KAGGLE_SYMLINK='kaggle'\n",
        "\n",
        "!umount /kaggle/input/ 2> /dev/null\n",
        "shutil.rmtree('/kaggle/input', ignore_errors=True)\n",
        "os.makedirs(KAGGLE_INPUT_PATH, 0o777, exist_ok=True)\n",
        "os.makedirs(KAGGLE_WORKING_PATH, 0o777, exist_ok=True)\n",
        "\n",
        "try:\n",
        "  os.symlink(KAGGLE_INPUT_PATH, os.path.join(\"..\", 'input'), target_is_directory=True)\n",
        "except FileExistsError:\n",
        "  pass\n",
        "try:\n",
        "  os.symlink(KAGGLE_WORKING_PATH, os.path.join(\"..\", 'working'), target_is_directory=True)\n",
        "except FileExistsError:\n",
        "  pass\n",
        "\n",
        "for data_source_mapping in DATA_SOURCE_MAPPING.split(','):\n",
        "    directory, download_url_encoded = data_source_mapping.split(':')\n",
        "    download_url = unquote(download_url_encoded)\n",
        "    filename = urlparse(download_url).path\n",
        "    destination_path = os.path.join(KAGGLE_INPUT_PATH, directory)\n",
        "    try:\n",
        "        with urlopen(download_url) as fileres, NamedTemporaryFile() as tfile:\n",
        "            total_length = fileres.headers['content-length']\n",
        "            print(f'Downloading {directory}, {total_length} bytes compressed')\n",
        "            dl = 0\n",
        "            data = fileres.read(CHUNK_SIZE)\n",
        "            while len(data) > 0:\n",
        "                dl += len(data)\n",
        "                tfile.write(data)\n",
        "                done = int(50 * dl / int(total_length))\n",
        "                sys.stdout.write(f\"\\r[{'=' * done}{' ' * (50-done)}] {dl} bytes downloaded\")\n",
        "                sys.stdout.flush()\n",
        "                data = fileres.read(CHUNK_SIZE)\n",
        "            if filename.endswith('.zip'):\n",
        "              with ZipFile(tfile) as zfile:\n",
        "                zfile.extractall(destination_path)\n",
        "            else:\n",
        "              with tarfile.open(tfile.name) as tarfile:\n",
        "                tarfile.extractall(destination_path)\n",
        "            print(f'\\nDownloaded and uncompressed: {directory}')\n",
        "    except HTTPError as e:\n",
        "        print(f'Failed to load (likely expired) {download_url} to path {destination_path}')\n",
        "        continue\n",
        "    except OSError as e:\n",
        "        print(f'Failed to load {download_url} to path {destination_path}')\n",
        "        continue\n",
        "\n",
        "print('Data source import complete.')\n"
      ],
      "metadata": {
        "id": "G3enzi4FGnR1"
      },
      "cell_type": "code",
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# This Python 3 environment comes with many helpful analytics libraries installed\n",
        "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
        "# For example, here's several helpful packages to load\n",
        "\n",
        "import numpy as np # linear algebra\n",
        "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
        "\n",
        "# Input data files are available in the read-only \"../input/\" directory\n",
        "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
        "\n",
        "import os\n",
        "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
        "    for filename in filenames:\n",
        "        print(os.path.join(dirname, filename))\n",
        "\n",
        "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\"\n",
        "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
      ],
      "metadata": {
        "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
        "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
        "execution": {
          "iopub.status.busy": "2024-07-09T14:37:26.697869Z",
          "iopub.execute_input": "2024-07-09T14:37:26.698223Z",
          "iopub.status.idle": "2024-07-09T14:37:27.731907Z",
          "shell.execute_reply.started": "2024-07-09T14:37:26.69819Z",
          "shell.execute_reply": "2024-07-09T14:37:27.730922Z"
        },
        "trusted": true,
        "id": "2jqXy0Z9GnR2",
        "outputId": "233e6656-a49d-4863-8ff7-61e5e9b2f983"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "text": "/kaggle/input/gggsdsd/balanced.csv\n",
          "output_type": "stream"
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "df = pd.read_csv('/kaggle/input/gggsdsd/balanced.csv',encoding='utf-8')\n",
        "df.head()"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-07-09T14:38:20.282718Z",
          "iopub.execute_input": "2024-07-09T14:38:20.283107Z",
          "iopub.status.idle": "2024-07-09T14:38:20.560827Z",
          "shell.execute_reply.started": "2024-07-09T14:38:20.283075Z",
          "shell.execute_reply": "2024-07-09T14:38:20.559802Z"
        },
        "trusted": true,
        "id": "xt_-_Gw7GnR4",
        "outputId": "b0d67346-631a-4edb-f978-50efc3dc5519"
      },
      "execution_count": null,
      "outputs": [
        {
          "execution_count": 4,
          "output_type": "execute_result",
          "data": {
            "text/plain": "                                               texts  label\n0  अमेरिकी और रूसी पुलिस के संयुक्त अभियान के फलस...      0\n1  आज यहां रांची एक्सप्रेस के साथ बातचीत में वित्...      0\n2                                   हमे इन्तजार है ।      1\n3       जिस के नतीजे मे ये हुक्म सादिर किया गया है ।      1\n4                                              जयपुर      0",
            "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>texts</th>\n      <th>label</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>अमेरिकी और रूसी पुलिस के संयुक्त अभियान के फलस...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>आज यहां रांची एक्सप्रेस के साथ बातचीत में वित्...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>हमे इन्तजार है ।</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>जिस के नतीजे मे ये हुक्म सादिर किया गया है ।</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>जयपुर</td>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install stanza"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-07-09T14:38:22.443236Z",
          "iopub.execute_input": "2024-07-09T14:38:22.443938Z",
          "iopub.status.idle": "2024-07-09T14:38:37.094488Z",
          "shell.execute_reply.started": "2024-07-09T14:38:22.443906Z",
          "shell.execute_reply": "2024-07-09T14:38:37.093365Z"
        },
        "trusted": true,
        "id": "TFGJCKrUGnR5",
        "outputId": "402c2962-9120-4c27-ad4b-117055997e25"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "text": "Collecting stanza\n  Downloading stanza-1.8.2-py3-none-any.whl.metadata (13 kB)\nRequirement already satisfied: emoji in /opt/conda/lib/python3.10/site-packages (from stanza) (2.12.1)\nRequirement already satisfied: numpy in /opt/conda/lib/python3.10/site-packages (from stanza) (1.26.4)\nRequirement already satisfied: protobuf>=3.15.0 in /opt/conda/lib/python3.10/site-packages (from stanza) (3.20.3)\nRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from stanza) (2.32.3)\nRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from stanza) (3.2.1)\nRequirement already satisfied: toml in /opt/conda/lib/python3.10/site-packages (from stanza) (0.10.2)\nRequirement already satisfied: torch>=1.3.0 in /opt/conda/lib/python3.10/site-packages (from stanza) (2.1.2)\nRequirement already satisfied: tqdm in /opt/conda/lib/python3.10/site-packages (from stanza) (4.66.4)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from torch>=1.3.0->stanza) (3.13.1)\nRequirement already satisfied: typing-extensions in /opt/conda/lib/python3.10/site-packages (from torch>=1.3.0->stanza) (4.9.0)\nRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch>=1.3.0->stanza) (1.12.1)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch>=1.3.0->stanza) (3.1.2)\nRequirement already satisfied: fsspec in /opt/conda/lib/python3.10/site-packages (from torch>=1.3.0->stanza) (2024.3.1)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->stanza) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->stanza) (3.6)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->stanza) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->stanza) (2024.2.2)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch>=1.3.0->stanza) (2.1.3)\nRequirement already satisfied: mpmath<1.4.0,>=1.1.0 in /opt/conda/lib/python3.10/site-packages (from sympy->torch>=1.3.0->stanza) (1.3.0)\nDownloading stanza-1.8.2-py3-none-any.whl (990 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m990.1/990.1 kB\u001b[0m \u001b[31m13.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m0:01\u001b[0m\n\u001b[?25hInstalling collected packages: stanza\nSuccessfully installed stanza-1.8.2\n",
          "output_type": "stream"
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import stanza\n",
        "stanza.download('hi')\n",
        "nlp = stanza.Pipeline('hi')\n",
        "\n",
        "def get_pos_tags(sentence):\n",
        "    doc = nlp(sentence)\n",
        "    return [word.upos for sent in doc.sentences for word in sent.words]\n",
        "\n",
        "df['pos'] = df['texts'].apply(get_pos_tags)\n",
        "print(df['pos'])"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-07-09T14:38:49.776816Z",
          "iopub.execute_input": "2024-07-09T14:38:49.777196Z",
          "iopub.status.idle": "2024-07-09T15:22:20.421313Z",
          "shell.execute_reply.started": "2024-07-09T14:38:49.777143Z",
          "shell.execute_reply": "2024-07-09T15:22:20.420305Z"
        },
        "trusted": true,
        "id": "wAKJVzlbGnR5",
        "outputId": "df9098d8-5133-45e3-f1e8-8f7761c97fe7",
        "colab": {
          "referenced_widgets": [
            "5086bc64bf5d4015a4ce65b4db7fa467",
            "4c79b994bcb44a1fb1205a792638a2ad",
            "057e747bcafc4e0389b6f7f159305684"
          ]
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.8.0.json:   0%|   …",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "5086bc64bf5d4015a4ce65b4db7fa467"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "Downloading https://huggingface.co/stanfordnlp/stanza-hi/resolve/v1.8.0/models/default.zip:   0%|          | 0…",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "4c79b994bcb44a1fb1205a792638a2ad"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.8.0.json:   0%|   …",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "057e747bcafc4e0389b6f7f159305684"
            }
          },
          "metadata": {}
        },
        {
          "name": "stdout",
          "text": "0        [ADJ, CCONJ, ADJ, NOUN, ADP, ADJ, NOUN, ADP, A...\n1        [NOUN, PRON, PROPN, PROPN, ADP, ADP, NOUN, ADP...\n2                                 [PRON, NOUN, AUX, PUNCT]\n3        [PRON, ADP, NOUN, ADP, DET, NOUN, NOUN, VERB, ...\n4                                                  [PROPN]\n                               ...                        \n38683    [NOUN, ADP, NOUN, PROPN, ADP, VERB, SCONJ, PRO...\n38684    [PROPN, ADP, ADJ, NOUN, ADP, NOUN, ADP, NOUN, ...\n38685                               [AUX, AUX, AUX, PUNCT]\n38686    [PRON, PRON, PRON, VERB, AUX, SCONJ, ADV, NOUN...\n38687    [PRON, VERB, SCONJ, ADJ, ADV, NOUN, ADP, ADJ, ...\nName: pos, Length: 38688, dtype: object\n",
          "output_type": "stream"
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df.to_csv('df_pos.csv', index=False)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-07-09T15:22:22.536512Z",
          "iopub.execute_input": "2024-07-09T15:22:22.536802Z",
          "iopub.status.idle": "2024-07-09T15:22:23.258433Z",
          "shell.execute_reply.started": "2024-07-09T15:22:22.536778Z",
          "shell.execute_reply": "2024-07-09T15:22:23.25743Z"
        },
        "trusted": true,
        "id": "oxPrI3mtGnR6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "DMYhj1sUGnR6"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}